import faiss
import pickle
import numpy as np
from sentence_transformers import SentenceTransformer
from typing import List, Dict, Optional
import logging
from functools import lru_cache

class OptimizedRetrieverAgent:
    def __init__(self, index_path, metadata_path, model_name="all-MiniLM-L6-v2"):
        self.index = faiss.read_index(index_path)
        with open(metadata_path, "rb") as f:
            self.meta = pickle.load(f)
        self.model = SentenceTransformer(model_name)
        self.chunk_cache = self._load_all_chunks()

        self.query_patterns = {
            'simple': ['what is', 'define', 'explain'],
            'complex': ['how to', 'process', 'steps', 'requirements'],
            'comparison': ['vs', 'versus', 'difference', 'compare'],
            'policy': ['policy', 'rule', 'regulation', 'allowed']
        }

    def _load_all_chunks(self) -> Dict[int, str]:
        cache = {}
        for i, meta_item in enumerate(self.meta):
            chunk_file = meta_item['source_file'].replace('.pdf', f"_chunk_{meta_item['chunk_id']}.txt")
            try:
                with open(f"chunks/{chunk_file}", "r") as f:
                    cache[i] = f.read().strip()
            except Exception as e:
                logging.warning(f"âŒ Failed to load chunk {i}: {e}")
                cache[i] = ""
        return cache

    def _classify_query(self, question: str) -> str:
        q = question.lower()
        for qtype, patterns in self.query_patterns.items():
            if any(p in q for p in patterns):
                return qtype
        return 'general'

    def _get_dynamic_k(self, query_type: str, question: str) -> int:
        if query_type == 'simple':
            return 2
        elif query_type == 'complex':
            return 7
        elif len(question.split()) > 15:
            return 6
        return 4

    @lru_cache(maxsize=128)
    def _cached_embedding(self, question: str):
        return self.model.encode([question])

    def retrieve(self, question: str, top_k: Optional[int] = None) -> List[Dict]:
        query_type = self._classify_query(question)
        k = top_k or self._get_dynamic_k(query_type, question)

        q_embedding = self._cached_embedding(question)
        distances, indices = self.index.search(np.array(q_embedding), k)

        results = []
        for idx in indices[0]:
            if idx < len(self.meta):
                results.append({
                    "source_file": self.meta[idx]['source_file'],
                    "category": self.meta[idx]['category'],
                    "chunk_id": self.meta[idx]['chunk_id'],
                    "text": self.chunk_cache.get(idx, ""),
                    "distance": float(distances[0][len(results)])
                })
        return results
